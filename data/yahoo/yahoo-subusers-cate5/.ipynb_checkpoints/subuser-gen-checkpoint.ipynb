{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3f0e671",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T11:36:57.413921Z",
     "start_time": "2023-03-26T11:36:55.645197Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyper-parameters\n",
    "user_clip_num = 2\n",
    "user_pos_num_clip = 2 # only split users with training positive interactions >= user_clip_num * user_pos_num_clip\n",
    "max_interact_num = 10\n",
    "like_threshold = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce855e6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T11:37:02.772331Z",
     "start_time": "2023-03-26T11:36:57.417927Z"
    }
   },
   "outputs": [],
   "source": [
    "data_string = open('../yahoo-origin/user.txt', 'r', encoding = \"ISO-8859-1\").read().strip().split('\\n')\n",
    "data_dict = {\"user_id\":[],\"item_id\":[],\"score\":[]}\n",
    "for da in data_string:\n",
    "    l_string = da.split(',')\n",
    "    data_dict[\"user_id\"].append(eval(l_string[0]))\n",
    "    data_dict[\"item_id\"].append(eval(l_string[1]))\n",
    "    data_dict[\"score\"].append(eval(l_string[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5adb03e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T11:37:02.788332Z",
     "start_time": "2023-03-26T11:37:02.775332Z"
    }
   },
   "outputs": [],
   "source": [
    "path='./features'\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "path='./sets'\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9e74cd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T11:37:03.099355Z",
     "start_time": "2023-03-26T11:37:02.796333Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(311704, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_raw = pd.DataFrame(data_dict)\n",
    "train_data_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ddc0564",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T11:37:03.145356Z",
     "start_time": "2023-03-26T11:37:03.101354Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125077, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_pos = train_data_raw[train_data_raw.score>3]\n",
    "train_data_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "729fd790",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T11:37:03.175776Z",
     "start_time": "2023-03-26T11:37:03.148358Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id    14382\n",
       "item_id     1000\n",
       "score          2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_pos.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4cc0c64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T11:37:04.151873Z",
     "start_time": "2023-03-26T11:37:03.178781Z"
    }
   },
   "outputs": [],
   "source": [
    "data_string2 = open('../yahoo-origin/random.txt', 'r', encoding = \"ISO-8859-1\").read().strip().split('\\n')\n",
    "data_dict2 = {\"user_id\":[],\"item_id\":[],\"score\":[]}\n",
    "for da in data_string2:\n",
    "    l_string = da.split(',')\n",
    "    data_dict2[\"user_id\"].append(eval(l_string[0]))\n",
    "    data_dict2[\"item_id\"].append(eval(l_string[1]))\n",
    "    data_dict2[\"score\"].append(eval(l_string[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cb24e94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T11:37:04.214885Z",
     "start_time": "2023-03-26T11:37:04.153873Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54000, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_raw = pd.DataFrame(data_dict2)\n",
    "test_data_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ddce89a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T11:37:20.297110Z",
     "start_time": "2023-03-26T11:37:04.217891Z"
    }
   },
   "outputs": [],
   "source": [
    "for id in test_data_raw.user_id.unique():\n",
    "    if id not in train_data_pos.user_id.unique():\n",
    "        test_data_raw = test_data_raw[test_data_raw.user_id!=id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bb9013a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T11:37:20.312138Z",
     "start_time": "2023-03-26T11:37:20.299126Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50590, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c5383c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T11:37:20.343128Z",
     "start_time": "2023-03-26T11:37:20.316130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4671, 3)\n",
      "2383\n"
     ]
    }
   ],
   "source": [
    "test_data_pos = test_data_raw[test_data_raw.score>3]\n",
    "print(test_data_pos.shape)\n",
    "print(test_data_pos.user_id.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "504344ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T11:37:54.569959Z",
     "start_time": "2023-03-26T11:37:20.346132Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 125077/125077 [00:34<00:00, 3659.84it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "item_id_map = {}\n",
    "user_id_map = {}\n",
    "userID = 0\n",
    "itemID = 0\n",
    "big_pos_list = []\n",
    "train_data = train_data_pos.copy(deep=True)\n",
    "for i in tqdm(range(train_data.shape[0])):\n",
    "    if train_data.iloc[i,0] in user_id_map:\n",
    "        train_data.iloc[i,0] = user_id_map[train_data.iloc[i,0]]\n",
    "    else:\n",
    "        uID = userID\n",
    "        user_id_map[train_data.iloc[i,0]]=uID\n",
    "        train_data.iloc[i,0] = uID\n",
    "        userID += 1\n",
    "    #print(train_data.iloc[i,1])\n",
    "    if train_data.iloc[i,1] in item_id_map:\n",
    "        # print(\"train_data.iloc[i,1]\",train_data.iloc[i,1])\n",
    "        # print(\"item_id_map:\",item_id_map)\n",
    "        train_data.iloc[i,1] = item_id_map[train_data.iloc[i,1]]\n",
    "    else:\n",
    "        iID = itemID\n",
    "        item_id_map[train_data.iloc[i,1]]=iID\n",
    "        train_data.iloc[i,1] = iID\n",
    "        itemID += 1  \n",
    "# train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6524bd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T11:37:54.600967Z",
     "start_time": "2023-03-26T11:37:54.574968Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311699</th>\n",
       "      <td>15399</td>\n",
       "      <td>563</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311700</th>\n",
       "      <td>15399</td>\n",
       "      <td>577</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311701</th>\n",
       "      <td>15399</td>\n",
       "      <td>636</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311702</th>\n",
       "      <td>15399</td>\n",
       "      <td>883</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311703</th>\n",
       "      <td>15399</td>\n",
       "      <td>948</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>311704 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id  item_id  score\n",
       "0             0       13      5\n",
       "1             0       34      1\n",
       "2             0       45      1\n",
       "3             0       82      1\n",
       "4             0       92      1\n",
       "...         ...      ...    ...\n",
       "311699    15399      563      5\n",
       "311700    15399      577      1\n",
       "311701    15399      636      5\n",
       "311702    15399      883      1\n",
       "311703    15399      948      5\n",
       "\n",
       "[311704 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76057c6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T11:38:51.455261Z",
     "start_time": "2023-03-26T11:37:54.604970Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 311704/311704 [00:56<00:00, 5485.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# added for splitting users: only split training data, keep testing data untouched.\n",
    "big_matrix = train_data_raw\n",
    "user_all_dict = {} # to record every exposure interaction with time and watch_ratio(here only with score)\n",
    "for i in tqdm(range(big_matrix.shape[0])):\n",
    "    if big_matrix.iloc[i,0] in user_id_map:\n",
    "        uID = user_id_map[big_matrix.iloc[i,0]]\n",
    "        \n",
    "        # this is a recorded positive interaction\n",
    "        if big_matrix.iloc[i,2]>=4: \n",
    "            iID = item_id_map[big_matrix.iloc[i,1]]\n",
    "#             assert [uID, iID] in big_pos_list # double check this is recorded\n",
    "            \n",
    "            if uID not in user_all_dict:\n",
    "                user_all_dict[uID] = {'exposure':[], 'like':[]}\n",
    "            user_all_dict[uID]['exposure'].append([iID, big_matrix.iloc[i,2]])\n",
    "            user_all_dict[uID]['like'].append([iID, big_matrix.iloc[i,2]])\n",
    "            \n",
    "        # this is a negative interaction\n",
    "        else:\n",
    "            if big_matrix.iloc[i,1] in item_id_map: \n",
    "                iID = item_id_map[big_matrix.iloc[i,1]]\n",
    "            # this is a new item unmapped\n",
    "            else: \n",
    "                iID = len(item_id_map)\n",
    "                item_id_map[big_matrix.iloc[i,1]] = iID\n",
    "                \n",
    "            if uID not in user_all_dict:\n",
    "                user_all_dict[uID] = {'exposure':[], 'like':[]}\n",
    "            user_all_dict[uID]['exposure'].append([iID, big_matrix.iloc[i,2]])\n",
    "    else:\n",
    "        continue # this user is deleted in previous MF data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a841954",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T11:38:57.452501Z",
     "start_time": "2023-03-26T11:38:51.458264Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\download\\anaconda\\envs\\myjupyter\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1332: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embed_item = np.load(\"./embed_item_MF_yahoo_512factor_num_0.0001lr_256bs_0.0dropout.npy\")\n",
    "\n",
    "import pandas as pd  \n",
    "from sklearn.cluster import KMeans  \n",
    "import matplotlib.pyplot as plt  \n",
    "from tqdm import tqdm\n",
    "category_num_new = 12\n",
    "kmeans_model = KMeans(category_num_new, random_state=1)\n",
    "kmeans_model.fit(embed_item)\n",
    "labels = kmeans_model.labels_\n",
    "\n",
    "# create item_feature_dict file CHANGED!!!\n",
    "item_feature_dict = {}\n",
    "for item in range(embed_item.shape[0]):\n",
    "    item_feature_dict[item] = [labels[item]]\n",
    "\n",
    "np.save('./features/item_feature_dict.npy', np.array(item_feature_dict))\n",
    "\n",
    "# create category_list file\n",
    "category_list = list(range(category_num_new))\n",
    "np.save('./features/category_list.npy', np.array(category_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b55e75f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T11:38:58.660593Z",
     "start_time": "2023-03-26T11:38:57.454502Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 4671/4671 [00:01<00:00, 3931.52it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data0 = test_data_pos.copy(deep=True)\n",
    "for i in tqdm(range(test_data0.shape[0])):\n",
    "    if test_data0.iloc[i,0] in user_id_map:\n",
    "        test_data0.iloc[i,0] = user_id_map[test_data0.iloc[i,0]]\n",
    "    else:\n",
    "        print('u',uID)\n",
    "    if test_data0.iloc[i,1] in item_id_map:\n",
    "        test_data0.iloc[i,1] = item_id_map[test_data0.iloc[i,1]]\n",
    "    else:\n",
    "        print('i',iID)\n",
    "# test_data0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b28fd1f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T11:39:01.117791Z",
     "start_time": "2023-03-26T11:38:58.663595Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dict=train_data.sort_values('user_id').groupby('user_id')\n",
    "train_user_id=train_data.user_id.unique()\n",
    "training_dict=dict()\n",
    "for i in train_user_id:\n",
    "    training_dict[i]=train_dict.get_group(i).item_id.values.tolist()\n",
    "training_list = []\n",
    "for u in training_dict:\n",
    "    for i in training_dict[u]:\n",
    "        training_list.append([u,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3331c4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T11:39:01.913852Z",
     "start_time": "2023-03-26T11:39:01.120793Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|█████████████████▍                                                         | 3343/14382 [00:00<00:01, 9950.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'D': [0, 1, 2, 3, 4, 5, 6, 7, 8], 'T': [0.0, 0.15, 0.0, 0.05, 0.0, 0.15, 0.05, 0.45, 0.0, 0.05, 0.1, 0.0], 'Y': [0, 0.0, 0, 1.0, 0, 0.3333333333333333, 0.0, 0.7777777777777778, 0, 0.0, 0.0, 0]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 14382/14382 [00:00<00:00, 18897.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# split users and generate new big_pos_dict and big_pos_list.\n",
    "big_pos_dict = dict(training_dict)\n",
    "big_pos_list = list(training_list)\n",
    "# a function to calc T given exposure items and item_category_dict&category_list\n",
    "def calc_T(t_seq, item_category, category_list):\n",
    "    cate_dis = [0] * (max(category_list)+1)\n",
    "    for item in t_seq:\n",
    "        for c in item_category[item]:\n",
    "            cate_dis[c] += 1/len(item_category[item])\n",
    "    cate_dis = [c/len(t_seq) for c in cate_dis]\n",
    "    return cate_dis\n",
    "\n",
    "# a function to calc Y given exposure items, liked items, and item_category_dict&category_list\n",
    "def calc_Y(t_seq, next_d, item_category, category_list):\n",
    "    exposure_dis = [0]* (max(category_list)+1)\n",
    "    like_dis = [0]* (max(category_list)+1)\n",
    "    for item in t_seq:\n",
    "        for c in item_category[item]:\n",
    "            exposure_dis[c] += 1/len(item_category[item])\n",
    "    for item in next_d:\n",
    "        for c in item_category[item]:\n",
    "            like_dis[c] += 1/len(item_category[item])\n",
    "    ctr_dis = [like_dis[c]/exposure_dis[c] if exposure_dis[c] > 0 else 0 for c in range(len(exposure_dis))]\n",
    "    return ctr_dis\n",
    "    \n",
    "subuser_all_dict = {} # subuserID\n",
    "user_subuser_map = {}\n",
    "user_subuser_exp_dict = {} # save exposure data of users and subusers\n",
    "user_num = len(user_id_map)\n",
    "assert user_num not in big_pos_dict\n",
    "\n",
    "for uID in tqdm(user_all_dict):\n",
    "    pos_num = len(user_all_dict[uID]['like'])\n",
    "    \n",
    "    # save exposure data of uID\n",
    "    user_subuser_exp_dict[uID] = [entry[0] for entry in user_all_dict[uID]['exposure']]\n",
    "    \n",
    "    # only split users with postive len > user_clip_num * user_pos_num_clip\n",
    "    if pos_num < user_clip_num * user_pos_num_clip:\n",
    "        continue\n",
    "    pos_num_clip = pos_num//user_clip_num\n",
    "    for i in range(user_clip_num-1):\n",
    "        \n",
    "        subuser_id = user_num\n",
    "        user_num += 1\n",
    "        user_subuser_map[subuser_id] = uID\n",
    "        subuser_all_dict[subuser_id] = {}\n",
    "        \n",
    "        # find like history in current clip\n",
    "        d = user_all_dict[uID]['like'][:(i+1)*pos_num_clip]\n",
    "        \n",
    "        # find treatment in next clip\n",
    "        st = user_all_dict[uID]['exposure'].index(user_all_dict[uID]['like'][(i+1)*pos_num_clip])\n",
    "        if i != user_clip_num-2: # not last clip\n",
    "            ed = user_all_dict[uID]['exposure'].index(user_all_dict[uID]['like'][(i+2)*pos_num_clip])\n",
    "            t_seq = user_all_dict[uID]['exposure'][st:ed]\n",
    "        else:\n",
    "            t_seq = user_all_dict[uID]['exposure'][st:]\n",
    "            \n",
    "        # find like history in next clip\n",
    "        if i != user_clip_num-2: # not last clip\n",
    "            next_d = user_all_dict[uID]['like'][(i+1)*pos_num_clip:(i+2)*pos_num_clip]\n",
    "        else:\n",
    "            next_d = user_all_dict[uID]['like'][(i+1)*pos_num_clip:]\n",
    "        \n",
    "        # save exposure data of subuser_id: find the last liked item and save its exposure.\n",
    "        exposure_index = user_all_dict[uID]['exposure'].index(d[-1])\n",
    "        user_subuser_exp_dict[subuser_id] = user_all_dict[uID]['exposure'][:exposure_index+1]\n",
    "        \n",
    "        d = [entry[0] for entry in d]\n",
    "        next_d = [entry[0] for entry in next_d]\n",
    "        t_seq = [entry[0] for entry in t_seq]\n",
    "        \n",
    "        T = calc_T(t_seq, item_feature_dict, category_list)\n",
    "        Y = calc_Y(t_seq, next_d, item_feature_dict, category_list)\n",
    "\n",
    "        subuser_all_dict[subuser_id]['D'] = d\n",
    "        subuser_all_dict[subuser_id]['T'] = T\n",
    "        subuser_all_dict[subuser_id]['Y'] = Y\n",
    "                \n",
    "        # update big_pos_dict and big_pos_list\n",
    "        for item in d:\n",
    "            if subuser_id not in big_pos_dict:\n",
    "                big_pos_dict[subuser_id] = []\n",
    "            big_pos_dict[subuser_id].append(item)\n",
    "            big_pos_list.append([subuser_id, item])\n",
    "\n",
    "        if uID == 0:\n",
    "            print(subuser_all_dict[subuser_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af2ba049",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T11:39:02.533915Z",
     "start_time": "2023-03-26T11:39:01.915854Z"
    }
   },
   "outputs": [],
   "source": [
    "np.save('./sets/training_dict.npy',np.array(big_pos_dict)) # 保存为.npy格式\n",
    "np.save('./sets/training_list.npy',np.array(big_pos_list)) # 保存为.npy格式\n",
    "np.save('./features/user_subuser_exp_dict.npy', np.array(user_subuser_exp_dict))\n",
    "np.save('./features/user_subuser_map.npy', np.array(user_subuser_map))\n",
    "np.save('./features/subuser_info.npy', np.array(subuser_all_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c1404ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T11:39:02.549916Z",
     "start_time": "2023-03-26T11:39:02.535917Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "def split_pandas_data_with_ratios(data, ratios, shuffle=False):\n",
    "    \"\"\"\n",
    "    按输入的比例列表对数据进行切分\n",
    "    Args:\n",
    "        data (pd.DataFrame): Pandas DataFrame 格式的数据.\n",
    "        ratios (list of floats): 切分比例的列表，和需要是 1.\n",
    "        shuffle (bool): 是否需要洗牌.\n",
    "\n",
    "    Returns:\n",
    "        list: 切分后的数据列表，列表中的元素类型为 pd.DataFrame .\n",
    "    \"\"\"\n",
    "    # 检查 切分比例的列表是否和为 1\n",
    "    if math.fsum(ratios) != 1.0:\n",
    "        raise ValueError(\"切分比例需要其和为1\")\n",
    "        \n",
    "    # 累加求和，例如 [0.7,0.15,0.15] 累计求和为 [0.7, 0.85, 1]\n",
    "    # 然后剔除最后一个值，即结果为 [0.7, 0.85]\n",
    "    split_index = np.cumsum(ratios).tolist()[:-1]\n",
    "    \n",
    "    # 是否洗牌\n",
    "    if shuffle:\n",
    "        # 不放回的从原始数据中随机取数据，直到到达frac设置的比例\n",
    "        data = data.sample(frac=1)\n",
    "    # 切分\n",
    "    #print(\"data_size:\",len(data))\n",
    "\n",
    "    splits = np.split(data, [round(x * len(data)) for x in split_index])\n",
    "\n",
    "    #对于valid为0的处理（只能处理训练集和验证集\n",
    "    # if(len(splits[0])==1):\n",
    "    #     splits[1]=splits[0].copy()\n",
    "\n",
    "    split_index_small=[ 0.5, 1 ]\n",
    "    if(len(splits[1])==0):\n",
    "        splits = np.split(data, [round(x * len(data)) for x in split_index_small])\n",
    "\n",
    "    for i in range(len(ratios)):\n",
    "        splits[i][\"split_index\"] = i\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f24c9d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T11:39:04.222046Z",
     "start_time": "2023-03-26T11:39:02.551920Z"
    }
   },
   "outputs": [],
   "source": [
    "df_grouped = test_data0.groupby('user_id')\n",
    "splits = []\n",
    "ratio=[0.5,0.5]\n",
    "\n",
    "# 对于每一名用户\n",
    "for name, group in df_grouped:\n",
    "    if(df_grouped.get_group(name).shape[0]<2):\n",
    "        continue\n",
    "    group_splits = split_pandas_data_with_ratios(df_grouped.get_group(name), ratio)\n",
    "    # 把切分好的数据再合并，数据值只多了 split_index 这一列\n",
    "    concat_group_splits = pd.concat(group_splits)\n",
    "    splits.append(concat_group_splits)\n",
    "\n",
    "# 把所有用户的数据合并\n",
    "splits_all = pd.concat(splits)\n",
    "\n",
    "# 按照 split_index 标记把数据切分出来\n",
    "splits_list = [\n",
    "    splits_all[splits_all[\"split_index\"] == x].drop(\"split_index\", axis=1)\n",
    "    for x in range(len(ratio))\n",
    "]\n",
    "valid_data=splits_list[0]\n",
    "test_data=splits_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "821243d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T11:39:04.270054Z",
     "start_time": "2023-03-26T11:39:04.226061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "train_user_id=train_data.user_id.unique()\n",
    "validation_id=valid_data.user_id.unique()\n",
    "test_id=test_data.user_id.unique()\n",
    "print(list(validation_id)==list(test_id))\n",
    "for i in validation_id:\n",
    "    if i not in train_user_id:\n",
    "        print(i,\" not in training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33c8f145",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T11:39:04.489082Z",
     "start_time": "2023-03-26T11:39:04.273059Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_dict=valid_data.groupby('user_id')\n",
    "valid_user_id=valid_data.sort_values('user_id').user_id.unique()\n",
    "validation_dict=dict()\n",
    "for i in valid_user_id:\n",
    "    validation_dict[i]=valid_dict.get_group(i).item_id.values.tolist()\n",
    "np.save('./sets/validation_dict.npy',np.array(validation_dict)) # 保存为.npy格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39d89d3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T11:39:04.735106Z",
     "start_time": "2023-03-26T11:39:04.492084Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dict=test_data.groupby('user_id')\n",
    "test_user_id=test_data.sort_values('user_id').user_id.unique()\n",
    "testing_dict=dict()\n",
    "for i in test_user_id:\n",
    "    testing_dict[i]=test_dict.get_group(i).item_id.values.tolist()\n",
    "np.save('./sets/testing_dict.npy',np.array(testing_dict)) # 保存为.npy格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fdef6218",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T11:39:04.861122Z",
     "start_time": "2023-03-26T11:39:04.739118Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "pop_rank = dict(pd.DataFrame(training_list)[1].value_counts())\n",
    "print(len(pop_rank))\n",
    "for item in item_id_map:\n",
    "    i = item_id_map[item]\n",
    "    if i not in pop_rank:\n",
    "        pop_rank[i] = 0\n",
    "print(len(pop_rank))\n",
    "\n",
    "item_rank_dict = {} #id:rank\n",
    "rank = 0\n",
    "for element in pop_rank:\n",
    "    item_rank_dict[element] = rank\n",
    "    rank += 1\n",
    "np.save('./features/item_rank_dict.npy', np.array(item_rank_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c395062",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T11:39:05.310874Z",
     "start_time": "2023-03-26T11:39:04.864124Z"
    }
   },
   "outputs": [],
   "source": [
    "category_num = len(category_list)\n",
    "topk_category = 3\n",
    "user_fml_cat_big = {}\n",
    "for user in training_dict:\n",
    "    category_cnt = [0] * category_num\n",
    "    for item in training_dict[user]:\n",
    "        for cat in item_feature_dict[item]:\n",
    "            category_cnt[cat] += 1\n",
    "    sorted_list = list(np.argsort(category_cnt))\n",
    "    fml_cat = sorted_list[-topk_category:]\n",
    "    user_fml_cat_big[user] = fml_cat\n",
    "np.save('./features/user_fml_cat.npy', np.array(user_fml_cat_big))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a509c94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T11:39:06.033964Z",
     "start_time": "2023-03-26T11:39:05.313881Z"
    }
   },
   "outputs": [],
   "source": [
    "category_num = len(category_list)\n",
    "user_fml_cat_big = {}\n",
    "for user in big_pos_dict:\n",
    "    category_cnt = [0] * category_num\n",
    "    for item in big_pos_dict[user]:\n",
    "        #print(item_feature_dict[item])\n",
    "        for cat in item_feature_dict[item]:\n",
    "            category_cnt[cat] += 1/len(item_feature_dict[item])\n",
    "    pscore = (np.array(category_cnt) / max(category_cnt)) #** 0.5\n",
    "    user_fml_cat_big[user] = pscore.tolist()\n",
    "np.save('./features/IPS.npy', np.array(user_fml_cat_big))\n",
    "np.save('./features/IPS_item_cal.npy', np.array(item_feature_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8be07415",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T11:39:06.290062Z",
     "start_time": "2023-03-26T11:39:06.036971Z"
    }
   },
   "outputs": [],
   "source": [
    "user_feature_file = {}\n",
    "for userID in big_pos_dict:\n",
    "    user_feature_file[userID] = [['U'+str(userID)]]\n",
    "    user_feature_file[userID].append([str(1)]*len(user_feature_file[userID][0]))\n",
    "\n",
    "item_feature_file = {}\n",
    "for itemID in item_feature_dict:\n",
    "    item_feature_file[itemID] = [['I'+str(itemID)]]\n",
    "    item_feature_file[itemID].append(['1'])\n",
    "    for cate in range(0, len(category_list)):\n",
    "        if cate in item_feature_dict[itemID]:\n",
    "            item_feature_file[itemID][0].append('IC'+str(cate))\n",
    "            item_feature_file[itemID][1].append(str(round(1.0/len(item_feature_dict[itemID]), 2)))\n",
    "        else:\n",
    "            item_feature_file[itemID][0].append('IC'+str(cate))\n",
    "            item_feature_file[itemID][1].append(str(0))\n",
    "np.save('./features/user_feature_file.npy', np.array(user_feature_file))\n",
    "np.save('./features/item_feature_file.npy', np.array(item_feature_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d26b6e91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T11:39:06.321390Z",
     "start_time": "2023-03-26T11:39:06.293064Z"
    }
   },
   "outputs": [],
   "source": [
    "item_cat_vec = {}\n",
    "for itemID in item_feature_dict: \n",
    "    item_cat_vec[itemID] = [0] * category_num\n",
    "    cat_num = len(item_feature_dict[itemID])\n",
    "    for cat in item_feature_dict[itemID]:\n",
    "        item_cat_vec[itemID][cat] = 1 / np.sqrt(cat_num)\n",
    "np.save('./features/item_cat_vec.npy', np.array(item_cat_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83398b30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
